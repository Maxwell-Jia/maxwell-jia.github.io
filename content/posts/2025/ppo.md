---
title: "Recontextualizing Proximal Policy Optimization (PPO) for LLM Alignment"
date: 2025-04-05
draft: false
tags: ["RLHF"]
hideToc: false
math: true
---

TD;DR: TODO

**Note:** Before we divide into details, it's important to have a basic understanding  of reinforcement learning and PPO algorithm.

## Theory

### Notation

To effectively discuss PPO in the context of LLM, let's first establish some key notations and components:

- $Q$: This represents our training dataset, where each $q \in Q$ serves as a prompt for the LLM.
- $o = \lbrace o_1, o_2, \ldots, o_{|o|} \rbrace$: This denotes the sequence of tokens generated by the LLM in response to the prompt $q$.
- $\pi _\theta (o _{t+1} | q, o _{<t})$: This represents the probability distribution of the policy model for generating the next token $o _{t+1}$ given the prompt $q$ and previously generated tokens $o _{<t}$.
- $V _\phi (q, o _{<t})$: This is our value model which estimates the expected returns at each step.
- $R(q, o)$: This is the reward model that evaluates the quality of generated responses.
- $\pi _{\theta _{old}}$: This refers to the policy model from the previous iteration.
- $\pi _{\theta _{ref}}$: This is the reference policy model, typically the initial pre-trained model, which helps prevent excessive deviation during training.

In the PPO framework for LLMs, the reward model remains fixed throughout the training process, as it's usually pre-trained before we begin the PPO phase. During the training, our focus is on optimizing two key components: the policy model $\pi _\theta$ and the value model $V _\phi$.

### Optimize Policy Model

To optimize parameters of the policy model, we follow a specific mathematical formulation:

$$
\theta ^* = \arg \max _\theta \mathbb{E} _{[q \sim Q, o \sim \pi _{\theta _{old}}]} \frac{1}{|o|} \sum _{t=1}^{|o|} \min \large [\frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})} A_t, \text{clip}( \frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})}, 1-\epsilon, 1+\epsilon) A_t \large ]
$$

In this formula, we use a "clip" function to limit the magnitude of the policy updates. The clip function is defined as:

$$
\text{clip}(x, l, r) = \min (r, \max (x, l))
$$

By constraining the policy ratio $\frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})}$ to the range $[1-\epsilon, 1+\epsilon]$, we prevent excessively large policy updates, which helps maintain training stability and prevents the model from diverging too quickly from its previous behavior.

Next, we calculate the advantage function $A_t$ using Generalized Advantage Estimation (GAE), which provides a better trade-off between bias and variance in policy gradient methods. The GAE is defined as:

$$
A_t = \delta_t + \gamma \lambda \delta_{t+1} + \gamma^2 \lambda^2 \delta _{t+2} + \ldots
$$

where $\delta_t$ is the temporal difference error, defined as:

$$
\delta_t = R(q, o_t) + \gamma V_\phi(q, o_{<t+1}) - V_\phi(q, o_{<t})
$$

In practice, we compute GAE recursively from the last token to the first:

$$
A_t = \delta_t + \gamma \lambda A_{t+1}
$$

Here $\gamma$ is the discount factor that reduces the weight of future rewards, and $\lambda$ is the GAE parameter that controls the trade-off between bias and variance. This formulation helps stabilize training by reducing the variance of advantage estimates while maintaining a reasonable bias level.

In addition to the primary objective of optimizing the policy model, we also seek to enhance the exploration capabilities of the policy model, enabling it to procuce a broader variety of outputs. To archieve this, we incorporate an entropy regularization term. Entropy measures the randomness of the policy model's distribution. A higher entropy value indicates a more diverse policy distribution, which allows the model to explore a wider array of possible sequences. For each token, entropy is calculated as:

$$
\text{E} = - \sum _{i=1}^n P_i \log P_i
$$

where $n$ represents the vocabulary size, and $P_i$ is the probability after applying the softmax function.

In practice, most of the $P_i$ values are close to 0, and this can lead to instability in computing $\log P_i$. To address this, we calculate the equivalent entropy as:

$$
\begin{aligned}
    \text{E} &= - \sum _{i=1}^n P_i \log P_i \\\\
    &= - \sum _{i=1}^n P_i \log (\frac{e^{z_i}}{\sum _{j=1}^n e^{z_j}}) \\\\
    &= - \sum _{i=1}^n P_i (z_i - \log \sum _{j=1}^n e^{z_j}) \\\\
    &= - \sum _{i=1}^n P_i z_i + \sum _{i=1}^n P_i \log \sum _{j=1}^n e^{z_j} \\\\
    &= - \sum _{i=1}^n P_i z_i + \log \sum _{i=1}^n e^{z_i}
\end{aligned}
$$

Here $z_i$ denotes the model output logit for the $i$-th token in the embedding vocabulary. This method avoids the direct computation of $\log P_i$, enhancing the numerical stability.

While we encourage policy exploration, we still do not want the policy model to deviate too much from its initial state. Before the ppo training, the policy model usually performs well having been pre-trained and fine-tuned on a large corpus.

Therefore, we add a KL divergence regularization term. The KL divergence is defined as:

$$
\text{D} _{KL} (\pi _{\theta _{ref}} || \pi _\theta) = \mathbb{E} _{o \sim \pi _{\theta _{ref}}} \frac{1}{|o|} \sum _{t=1}^{|o|} \log \frac{\pi _{\theta _{ref}} (o_t | q, o _{<t})}{\pi (o_t | q, o _{<t})}
$$

In this equation, $\pi _{\theta _{ref}}$ represents the reference policy, while $\pi _\theta$ is the current policy we are optimizing. By minimizing the KL divergence, we encourage the new policy to remain close to the reference policy, thus preventing drastic changes that could destabilize training. This regularization helps maintain a balance between exploration and exploitation, ensuring that the model can learn effectively while still leveraging the knowledge encoded in the reference policy.


### Optimize Value Model

n the PPO framework, the value model plays a crucial role in estimating the expected future rewards for each state. A well-optimized value model helps reduce the variance in policy updates and enhances training efficiency.

The parameters of the value model are optimized by minimizing the mean squared error between the predicted values and the target returns. This optimization is performed token by token, similar to the process used for policy optimization.

The optimization of the value model parameters can be expressed as:

$$
\phi ^* = \arg \min _\phi \mathbb{E} _{[q \sim Q, o \sim \pi _{\theta _{old}}]} \frac{1}{|o|} \sum _{t=1}^{|o|} \left[ (V _\phi (q, o _{<t}) - r_t)^2 \right]
$$

where $r_t$ is the target return, defined as:

$$
r_t = A_t + V _{\phi _{old}}(q, o _{<t})
$$

This approach enables the value model to better approximate the true value function, which in turn provides more accurate advantage estimates for policy optimization. It is important to note that we use the old value model ($V _{\phi _{old}}$) to compute the target returns.


## Implementation

TODO

We have get the priliminary knowledge of ppo in llm. Now let's implement the ppo training pipeline. I'll use pseudocode to show the training process.

Step 1, we initialize all the models we want to train.

```python
# load llm model
actor = load_policy_model()
# clone a ref model
ref_model = copy.deepcopy(actor)
# initialize value model
critic = load_value_model()
# define reward function
reward_fn = define_reward_function()
```

Step 2, we define the training loop. We first present the overall framework here so that we can have a comprehensive understanding of the training process.

```python
for epoch in range(total_epochs):
    for batch in dataloader:
        # 1. rollout
        # 2. compute reward
        # 3. update value model
        # 4. update policy model
```

Reference:
- https://huggingface.co/blog/NormalUhr/rlhf-pipeline