---
title: "Recontextualizing Proximal Policy Optimization (PPO) for LLM Alignment"
date: 2025-04-06
draft: false
tags: ["RLHF"]
hideToc: false
math: true
---

## TD;DR

This blog provides an overview of Proximal Policy Optimization (PPO) for reinforcement learning, detailing the optimization of policy and value models. Key concepts include Generalized Advantage Estimation (GAE), entropy regularization, and KL divergence, all aimed at ensuring stable and effective training. A pseudocode implementation of the PPO training pipeline is also presented.

**Note: Before we dive into the details, it's essential to have a basic understanding of reinforcement learning and the Proximal Policy Optimization (PPO) algorithm in non-LLM context.**

## Theory

### Notation

To effectively discuss PPO in the context of LLM, let's first establish some key notations and components:

- $Q$: This represents our training dataset, where each $q \in Q$ serves as a prompt for the LLM.
- $o = \lbrace o_1, o_2, \ldots, o_{|o|} \rbrace$: This denotes the sequence of tokens generated by the LLM in response to the prompt $q$.
- $\pi _\theta (o _{t+1} | q, o _{<t})$: This represents the probability distribution of the policy model for generating the next token $o _{t+1}$ given the prompt $q$ and previously generated tokens $o _{<t}$.
- $V _\phi (q, o _{<t})$: This is our value model which estimates the expected returns at each step.
- $R(q, o)$: This is the reward model that evaluates the quality of generated responses.
- $\pi _{\theta _{old}}$: This refers to the policy model from the previous iteration.
- $\pi _{\theta _{ref}}$: This is the reference policy model, typically the initial pre-trained model, which helps prevent excessive deviation during training.

In the PPO framework for LLMs, the reward model remains fixed throughout the training process, as it's usually pre-trained before we begin the PPO phase. During the training, our focus is on optimizing two key components: the policy model $\pi _\theta$ and the value model $V _\phi$.

### Optimize Policy Model

To optimize parameters of the policy model, we follow a specific mathematical formulation:

$$
\theta ^* = \arg \max _\theta \mathbb{E} _{[q \sim Q, o \sim \pi _{\theta _{old}}]} \frac{1}{|o|} \sum _{t=1}^{|o|} \min \large [\frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})} A_t, \text{clip}( \frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})}, 1-\epsilon, 1+\epsilon) A_t \large ]
$$

In this formula, we use a "clip" function to limit the magnitude of the policy updates. The clip function is defined as:

$$
\text{clip}(x, l, r) = \min (r, \max (x, l))
$$

By constraining the policy ratio $\frac{\pi _\theta (o_t | q, o _{< t})}{\pi _{\theta _{old}}(o_t | q, o _{< t})}$ to the range $[1-\epsilon, 1+\epsilon]$, we prevent excessively large policy updates, which helps maintain training stability and prevents the model from diverging too quickly from its previous behavior.

Next, we calculate the advantage function $A_t$ using Generalized Advantage Estimation (GAE), which provides a better trade-off between bias and variance in policy gradient methods. The GAE is defined as:

$$
A_t = \delta_t + \gamma \lambda \delta_{t+1} + \gamma^2 \lambda^2 \delta _{t+2} + \ldots
$$

where $\delta_t$ is the temporal difference error, defined as:

$$
\delta_t = R(q, o_t) + \gamma V_\phi(q, o_{<t+1}) - V_\phi(q, o_{<t})
$$

In practice, we compute GAE recursively from the last token to the first:

$$
A_t = \delta_t + \gamma \lambda A_{t+1}
$$

Here $\gamma$ is the discount factor that reduces the weight of future rewards, and $\lambda$ is the GAE parameter that controls the trade-off between bias and variance. This formulation helps stabilize training by reducing the variance of advantage estimates while maintaining a reasonable bias level.

In addition to the primary objective of optimizing the policy model, we also seek to enhance the exploration capabilities of the policy model, enabling it to procuce a broader variety of outputs. To archieve this, we incorporate an entropy regularization term. Entropy measures the randomness of the policy model's distribution. A higher entropy value indicates a more diverse policy distribution, which allows the model to explore a wider array of possible sequences. For each token, entropy is calculated as:

$$
\text{E} = - \sum _{i=1}^n P_i \log P_i
$$

where $n$ represents the vocabulary size, and $P_i$ is the probability after applying the softmax function.

In practice, most of the $P_i$ values are close to 0, and this can lead to instability in computing $\log P_i$. To address this, we calculate the equivalent entropy as:

$$
\begin{aligned}
    \text{E} &= - \sum _{i=1}^n P_i \log P_i \\\\
    &= - \sum _{i=1}^n P_i \log (\frac{e^{z_i}}{\sum _{j=1}^n e^{z_j}}) \\\\
    &= - \sum _{i=1}^n P_i (z_i - \log \sum _{j=1}^n e^{z_j}) \\\\
    &= - \sum _{i=1}^n P_i z_i + \sum _{i=1}^n P_i \log \sum _{j=1}^n e^{z_j} \\\\
    &= - \sum _{i=1}^n P_i z_i + \log \sum _{i=1}^n e^{z_i}
\end{aligned}
$$

Here $z_i$ denotes the model output logit for the $i$-th token in the embedding vocabulary. This method avoids the direct computation of $\log P_i$, enhancing the numerical stability.

While we encourage policy exploration, we still do not want the policy model to deviate too much from its initial state. Before the ppo training, the policy model usually performs well having been pre-trained and fine-tuned on a large corpus.

Therefore, we add a KL divergence regularization term. The KL divergence is defined as:

$$
\text{D} _{KL} (\pi _{\theta _{ref}} || \pi _\theta) = \mathbb{E} _{o \sim \pi _{\theta _{ref}}} \frac{1}{|o|} \sum _{t=1}^{|o|} \log \frac{\pi _{\theta _{ref}} (o_t | q, o _{<t})}{\pi (o_t | q, o _{<t})}
$$

In this equation, $\pi _{\theta _{ref}}$ represents the reference policy, while $\pi _\theta$ is the current policy we are optimizing. By minimizing the KL divergence, we encourage the new policy to remain close to the reference policy, thus preventing drastic changes that could destabilize training. This regularization helps maintain a balance between exploration and exploitation, ensuring that the model can learn effectively while still leveraging the knowledge encoded in the reference policy.


### Optimize Value Model

In the PPO framework, the value model plays a crucial role in estimating the expected future rewards for each state. A well-optimized value model helps reduce the variance in policy updates and enhances training efficiency.

The parameters of the value model are optimized by minimizing the mean squared error between the predicted values and the target returns. This optimization is performed token by token, similar to the process used for policy optimization.

The optimization of the value model parameters can be expressed as:

$$
\phi ^* = \arg \min _\phi \mathbb{E} _{[q \sim Q, o \sim \pi _{\theta _{old}}]} \frac{1}{|o|} \sum _{t=1}^{|o|} \left[ (V _\phi (q, o _{<t}) - r_t)^2 \right]
$$

where $r_t$ is the target return, defined as:

$$
r_t = A_t + V _{\phi _{old}}(q, o _{<t})
$$

This approach enables the value model to better approximate the true value function, which in turn provides more accurate advantage estimates for policy optimization. It is important to note that we use the old value model ($V _{\phi _{old}}$) to compute the target returns.


## Implementation

Now, let's implement the PPO training pipeline. I will use pseudocode to illustrate the training process.

### Pipeline Overview

We build the code from top to bottom, starting with the initialization of all the models and the data loader. For simplicity, we will use a rule-based reward function similar to what is done in Deepseek-R1. After that, we will present the overall training loop.

```python
# load policy model
actor = load_policy_model()
# clone a ref model
ref_model = copy.deepcopy(actor)
# initialize value model
critic = load_value_model()
# define reward function
reward_fn = define_reward_function()
# define dataloader
dataloader = load_dataloader()

# the main training loop
for epoch in range(total_epochs):
    for batch in dataloader:
        # 1. rollout
        gen_batch_output = actor.generate(batch)
        # we also compute the log probs of the old policy and ref policy
        old_log_probs = actor.compute_log_probs(gen_batch_output, batch)
        ref_log_probs = ref_model.compute_log_probs(gen_batch_output, batch)

        # 2. compute values
        values = critic.compute_values(gen_batch_output, batch)
        
        # 3. compute reward scores
        rewards = reward_fn(gen_batch_output, batch)
        
        # 4. compute advantages and returns
        advantages = compute_gae(rewards, values)
        returns = advantages + values  # Simplified version
        
        # 5. update value model
        for _ in range(value_updates_epochs):
            for micro_batch in gen_batch_output:
                value_loss = update_critic(critic, returns, micro_batch)
        
        # 6. update policy model
        for _ in range(policy_updates_epochs):
            for micro_batch in gen_batch_output:
                policy_loss, kl_penalty = update_actor(
                    actor, ref_model, critic, 
                    micro_batch, old_log_probs, 
                    advantages, returns
                )
```

### Advantage Computation

We use Generalized Advantage Estimation (GAE) to balance bias-variance tradeoff:

```python
def compute_gae(rewards, values, gamma=0.99, lam=0.95):
    deltas = rewards + gamma * values[1:] - values[:-1]
    advantages = []
    advantage = 0
    for delta in reversed(deltas):
        advantage = delta + gamma * lam * advantage
        advantages.insert(0, advantage)
    return advantages
```

### Value Model Update

The value model is updated as follows:

```python
def update_critic(critic, returns, batch):
    current_values = critic.compute_values(batch)
    loss = mse_loss(current_values, returns)
    loss.backward()
    optimizer_critic.step()
    return loss
```

### Policy Model Update

The policy model is updated as follows:

```python
def update_actor(actor, ref_model, critic, batch, old_log_probs, advantages):
    # Get new policy's log probabilities
    new_log_probs = actor.compute_log_probs(batch)
    
    # Compute probability ratio
    ratio = (new_log_probs - old_log_probs).exp()
    
    # Compute clipped surrogate loss
    clip_epsilon = 0.2
    surrogate1 = ratio * advantages
    surrogate2 = ratio.clamp(1-clip_epsilon, 1+clip_epsilon) * advantages
    policy_loss = -torch.min(surrogate1, surrogate2).mean()
    
    # Add entropy bonus
    entropy_bonus = 0.01 * compute_entropy(new_log_probs)
    
    # Add KL penalty against reference model
    kl_div = compute_kl_divergence(actor, ref_model, batch)
    kl_penalty = 0.1 * kl_div
    
    total_loss = policy_loss - entropy_bonus + kl_penalty
    total_loss.backward()
    optimizer_actor.step()
    
    return policy_loss, kl_penalty
```

## Reference

- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [PPO Algorithm](https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a)
- [Proximal Policy Optimization (PPO)](https://huggingface.co/blog/deep-rl-ppo)